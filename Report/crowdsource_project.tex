\documentclass[12pt,onecolumn,letterpaper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{cite}
%\usepackage[ampersand]{easylist}
\usepackage{listings}
\usepackage[margin=0.5in]{geometry}
%\usepackage{marginnote}

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\parindent{0.2in}

\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

\begin{document}

\begin{minipage}{0.5\textwidth}
\begin{flushleft}
    \subsection*{NAME: Tanmay Gupta}
\end{flushleft}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
\begin{flushright}
    \subsection*{NET ID: tgupta6}
\end{flushright}
\end{minipage}

\begin{minipage}{0.5\textwidth}
\begin{flushleft}
    \subsection*{NAME: Peter Maginnis}
\end{flushleft}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
\begin{flushright}
    \subsection*{NET ID: maginni1}
\end{flushright}
\end{minipage}\begin{center}
    \Large\textbf{IE598: Inference in Graphical Models}\\
    \textbf{Homework 3}
\end{center}

\section*{EM without prior}
We treat $\textbf{A}$ as our observed variables, $\textbf{t}$ as latent variables and $\textbf{p}$ as parameters. Then the joint distribution will be given as:
\begin{align*}
\mathbb{P}(\textbf{A},\textbf{t}|\textbf{p}) &= \mathbb{P}(\textbf{A}|\textbf{t},\textbf{p})\mathbb{P}(\textbf{t}|\textbf{p}) \\
&= \mathbb{P}(\textbf{A}|\textbf{t},\textbf{p})\mathbb{P}(\textbf{t}) \tag{Since true labels are independent of worker abilities} \\
&=\prod_{(i,j)\in E}\mathbb{P}(A_{ij}|t_i,p_j)\prod_{i}^{n}\mathbb{P}(t_i) \\
\end{align*}

\noindent
where,
\begin{align*}
\mathbb{P}(A_{ij}|t_i,p_j) &= p_j\mathbb{I}(A_{ij}=t_i)+(1-p_j)\mathbb{I}(A_{ij}=-t_i) \\
\mathbb{P}(t_i) &= 
\frac{3}{4}\mathbb{I}(t_i=1)+\frac{1}{4}\mathbb{I}(t_i=-1)
\end{align*}

\subsection*{E-Step:}
In the \textbf{E} step we find the distribution of the latent variables as a function of the observed variables and parameters estimated in the previous iteration.
\begin{align*}
\mathbb{P}(\textbf{t}|\textbf{A},\textbf{p}^{{\rm old}}) &= 
\prod_{i=1}^{n} \mathbb{P}(t_i|\textbf{A},\textbf{p}^{{\rm old}})
\end{align*}

\noindent
where,
\begin{align*}
\mathbb{P}(t_i|\textbf{A},\textbf{p}^{{\rm old}}) &=
\mathbb{P}(t_i|\textbf{A}_{i\partial i},\textbf{p}_{\partial i}^{{\rm old}}) \\
&= 
\frac{
\mathbb{P}(\textbf{A}_{i\partial i},\textbf{t}_i|\textbf{p}_{\partial i}^{{\rm old}})
} 	
{
\mathbb{P}(\textbf{A}_{i\partial i}|\textbf{p}_{\partial i}^{{\rm old}})
} \\
&=
\frac{
\mathbb{P}(t_i)\prod_{j\in \partial i} \mathbb{P}(A_{ij}|t_i,p_j^{{\rm old}})
}
{
\sum_{t_i} \mathbb{P}(t_i)\prod_{j\in \partial i} \mathbb{P}(A_{ij}|t_i,p_j^{{\rm old}})
} \\
&= \gamma_i(t_i)
\end{align*}
In the sequel, allowing for a slight abuse of notation, we need only refer to
\begin{align*}
  \gamma_i:=\gamma_i(1)&=\mathbb{P}(t_i=1|\textbf{A},\textbf{p}^{{\rm old}})\\
  &=\frac{\mathbb{P}(A_{i\partial i}|t_i=1,{\bf p})\mathbb{P}(t_i=1)}{\mathbb{P}(A_{i\partial i}|t_i=1,{\bf p})\mathbb{P}(t_i=1)+\mathbb{P}(A_{i\partial i}|t_i=-1,{\bf p})\mathbb{P}(t_i=-1)}\\
  &=\frac{\frac{3}{4}\prod_{j\in\partial i} p_j^{\mathbb{I}(A_{ij}=1)}(1-p_j)^{\mathbb{I}(A_{ij}=-1)}}{\frac{3}{4}\prod_{j\in\partial i} p_j^{\mathbb{I}(A_{ij}=1)}(1-p_j)^{\mathbb{I}(A_{ij}=-1)}+\frac{1}{4}\prod_{j\in\partial i} p_j^{\mathbb{I}(A_{ij}=-1)}(1-p_j)^{\mathbb{I}(A_{ij}=1)}}\\
  &=\frac{\frac{3}{4}\prod_{j\in\partial i} \left[\frac{1}{2}+\frac{2p_j-1}{2}A_{ij}\right]}{\frac{3}{4}\prod_{j\in\partial i} \left[\frac{1}{2}+\frac{2p_j-1}{2}A_{ij}\right]+\frac{1}{4}\prod_{j\in\partial i} \left[\frac{1}{2}-\frac{2p_j-1}{2}A_{ij}\right]}
\end{align*}
\subsection*{M-Step:}
In this we approximate the log likelihood of the observed data using the expected log likelihood of the observed and latent variables together where the expectation is with respect to the distribution of the latent variables computed in the \textbf{E} step
\begin{align*}
Q(\textbf{p}|\textbf{p}^{{\rm old}}) &= 
\mathbb{E}_{
{\bf \gamma}
}
\left[
\log{
\mathbb{P}(\textbf{A},\textbf{t}|\textbf{p})
}
\right] \\
&=
\mathbb{E}_{
\textbf{t}|\textbf{A},\textbf{p}^{{\rm old}}
}
\left[
\log{
\mathbb{P}(\textbf{A},\textbf{t}|\textbf{p})
}
\right] \\
&=
\sum_{\textbf{t}\in \{-1,1\}^n} 
\mathbb{P}(\textbf{t}|\textbf{A},\textbf{p}^{{\rm old}})
\log{
\mathbb{P}(\textbf{A},\textbf{t}|\textbf{p})
} \\
&= 
\sum_{\textbf{t}\in \{-1,1\}^n}
\left[
\left(
\prod_{i=1}^{n} \mathbb{P}(t_i|\textbf{A},\textbf{p}^{{\rm old}})
\right)
\sum_{i=1}^{n}
\left(
\log{\mathbb{P}(t_i|\textbf{p})} +
\sum_{j\in \partial i} 
\log \mathbb{P}(A_{ij}|t_i,p_j) 
\right)
\right] \\
&=
\sum_{i=1}^{n}
\sum_{t_i} \gamma_i(t_i) 
\left[
\log{\mathbb{P}(t_i|\textbf{p})} +
\sum_{j\in \partial i} 
\log \mathbb{P}(A_{ij}|t_i,p_j) 
\right] \\
&=
\sum_{i=1}^{n}
\Bigg\{\gamma_i 
\left[
\log{\mathbb{P}(t_i=1)} +
\sum_{j\in \partial i} 
\log \mathbb{P}(A_{ij}|t_i=1,p_j) 
\right]\\
&\qquad+(1-\gamma_i)
\left[
\log{\mathbb{P}(t_i=-1)} +
\sum_{j\in \partial i} 
\log \mathbb{P}(A_{ij}|t_i=-1,p_j) 
\right]
\Bigg\}
\end{align*}

\noindent
Now to maximize $Q(\textbf{p}|\textbf{p}^{{\rm old}})$ with respect to $\textbf{p}$ we set the derivatives to zero.
\begin{align*}
& \frac{\partial{Q}}{\partial{p_j}} = 0 \\
%\implies &
%\sum_{i\in \partial j} 
%\sum_{t_i} \gamma_i(t_i)
%\left(
%\frac{1}{p_j}\mathbb{I}(A_{ij}=t_i) -
%\frac{1}{1-p_j}\mathbb{I}(A_{ij}=-t_i)
%\right) 
%= 0 \\
\implies &
\sum_{i\in \partial j} 
\left[
\gamma_i
\left(
\frac{\mathbb{I}(A_{ij}=1)}{p_j} -
\frac{\mathbb{I}(A_{ij}=-1)}{1-p_j}
\right) +
\left(1-\gamma_i\right)
\left(
\frac{\mathbb{I}(A_{ij}=-1)}{p_j} -
\frac{\mathbb{I}(A_{ij}=1)}{1-p_j}
\right) 
\right]
= 0 \\
\implies &
\sum_{i\in \partial j} 
\left[
\left(
\gamma_i-p_j
\right) 
\mathbb{I}(A_{ij}=1) +
\left(
1-\gamma_i - p_j
\right) 
\mathbb{I}(A_{ij}=-1)
\right] 
= 0 \\
\implies &
p_j = 
\frac{1}{|\partial j|}
\sum_{i\in \partial j} 
\left[\gamma_i
\mathbb{I}(A_{ij}=1) +
\left(
1-\gamma_i
\right) 
\mathbb{I}(A_{ij}=-1)\right]
\end{align*}

\section*{EM with Beta Prior}

Instead, we seek to maximize the objective 
\begin{align*}
   Q({\bf p}|{\bf p}^{\rm old})&=\mathbb{E}_{\gamma}\left[\ln\mathbb{P}(\bf{A}|\bf{p})+\ln\mathbb{P({\bf p})}\right]\\
   &=\sum_{i=1}^n \left[\gamma_i\ln{\frac{3}{4}a_i+(1-\gamma_i)\ln\frac{1}{4}b_i}\right]+\sum_{j=1}^m\ln f\left(\frac{p_j-0.1}{0.9}\right),
\end{align*}
where 
\begin{align*}
a_i&:=\mathbb{P}(A_{i\partial i}|t_i=1,{\bf p})=\prod_{j\in\partial i} p_j^{\mathbb{I}(A_{ij}=1)}(1-p_j)^{\mathbb{I}(A_{ij}=-1)}\\
b_i&:=\mathbb{P}(A_{i\partial i}|t_i=-1,{\bf p})=\prod_{j\in\partial i} p_j^{\mathbb{I}(A_{ij}=-1)}(1-p_j)^{\mathbb{I}(A_{ij}=1)},
\end{align*}
and
\begin{equation*}
 f(z)=\frac{1}{B(\alpha,\beta)}(z)^{\alpha-1}(1-z)^{\beta-1}
\end{equation*}
We take the gradient
\begin{align*}
   \frac{\partial{Q}}{\partial{p_j}}&=\sum_{i\in\partial j}\left\{\gamma_i\left[\frac{\mathbb{I}(A_{ij}=1)}{p_j}-\frac{\mathbb{I}(A_{ij}=-1)}{1-p_j}\right]+(1-\gamma_i)\left[\frac{\mathbb{I}(A_{ij}=-1)}{p_j}-\frac{\mathbb{I}(A_{ij}=1)}{1-p_j}\right]\right\}+\frac{\alpha-1}{p_j-0.1}-\frac{\beta-1}{1-p_j}\\
   &=\sum_{i\in\partial j}\left\{\frac{\gamma_i\mathbb{I}(A_{ij}=1)+(1-\gamma_i)(\mathbb{I}(A_{ij}=-1))}{p_j}-\frac{(1-\gamma_i)\mathbb{I}(A_{ij}=1)+\gamma_i(\mathbb{I}(A_{ij}=-1))}{1-p_j}\right\}+\frac{\alpha-1}{p_j-0.1}-\frac{\beta-1}{1-p_j}\\
   &=\sum_{i\in\partial j}\left\{\frac{\gamma_iA_{ij}-A_{ij}+1}{p_j}-\frac{A_{ij}-\gamma_iA_{ij}+1}{1-p_j}\right\}+\frac{\alpha-1}{p_j-0.1}-\frac{\beta-1}{1-p_j}.
\end{align*}
We then solve for the critical point
\begin{align*}
\frac{\partial{Q}}{\partial{p_j}}=0
\end{align*}
which implies
\begin{align*}
  0=(p_j-0.1)\sum_{i\in\partial j}\left\{\gamma_i\mathbb{I}(A_{ij}=1)+(1-\gamma_i)\mathbb{I}(A_{ij}=-1)-p_j\right\}+(\alpha-1)p_j(1-p_j)-(\beta-1)p_j(p_j-0.1).
\end{align*}
Define
\begin{equation*}
  \lambda_j:=\frac{1}{|\partial j|}\sum_{i\in\partial j}\left[\gamma_i\mathbb{I}(A_{ij}=1)+(1-\gamma_i)\mathbb{I}(A_{ij}=-1)\right].
\end{equation*}
Then, we may more compactly write the quadratic equation
\begin{equation*}
  0=(p_j-0.1)|\partial j|[\lambda_j-p_j]+(\alpha-1)p_j(1-p_j)-(\beta-1)p_j(p_j-0.1).
\end{equation*}
Alternatively, suppose we approximated $p_j$ as $\operatorname{Beta}(\alpha,\beta)$, then the above becomes
\begin{align*}
  0&=|\partial j|(\lambda_j-p_j)+(\alpha-1)(1-p_j)-(\beta-1)p_j\\
  &\implies \lambda_j|\partial j|+\alpha-1=(|\partial j|+\alpha+\beta-2)p_j\\
  &\implies p_j=\frac{\sum_{i\in\partial j}\left[\gamma_i\mathbb{I}(A_{ij}=1)+(1-\gamma_i)\mathbb{I}(A_{ij}=-1)\right]+\alpha-1}{|\partial j|+\alpha+\beta-2}
\end{align*}
\end{document}
